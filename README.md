# 求职RL算法工程师 CPNT Sleep_king

[![MIT License](https://img.shields.io/badge/License-MIT-green.svg)](https://choosealicense.com/licenses/mit/)
[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![Contributions Welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/hujch23/Deep-Reinforcement-Learning/issues)

## 📚 目录

- [基础概念](#基础概念)
- [核心算法](#核心算法)


## 🚀 1. 基础概念  

### 1.1 强化学习的基本组成部分有哪些？基本特征有哪些？
  - Agent（智能体）
  - Environment（环境）
  -  State（状态）
  -   Action（动作）
  -   Reward（奖励）
  -   Policy（策略）
  -   有试错探索过程，即需要通过探索环境来获取对当前环境的理解
  -   强化学习中的智能体会从环境中获得延迟奖励
  -   强化学习的训练过程中时间非常重要，因为数据都是时间关联的，而不是像监督学习中的数据大部分是满足独立同分布的
  -   强化学习中智能体的动作会影响它从环境中得到的反馈

### 1.2 解释什么是回报（Return）和价值函数（Value Function）？  
  - 回报：从某一时刻开始，未来所有奖励的折扣总和  
  - 价值函数：从某个状态开始，遵循特定策略能够获得的期望回报  

### 1.3 全部可观测（full observability）、完全可观测（fully observed）和部分可观测（partially observed）?
  - 当智能体的状态与环境的状态等价时，我们就称这个环境是全部可观测的
  - 当智能体能够观察到环境的所有状态时，我们称这个环境是完全可观测的
  - 一般智能体不能观察到环境的所有状态时，我们称这个环境是部分可观测的

### 1.4 强化学习的使用场景有哪些呢？
  - 7个字总结就是“多序列决策问题”，或者说是对应的模型未知，需要通过学习逐渐逼近真实模型的问题。并且当前的动作会影响环境的状态，即具有马尔可夫性的问题。同时应满足所有状态是可重复到达的条件，即满足可学习条件

### 1.5 强化学习相对于监督学习为什么训练过程会更加困难？
  - 处理的大多是序列数据，其很难像监督学习的样本一样满足独立同分布条件
  - 有奖励的延迟，即智能体的动作作用在环境中时，环境对于智能体状态的奖励存在延迟，使得反馈不实时
  - 监督学习有正确的标签，模型可以通过标签修正自己的预测来更新模型，而强化学习相当于一个“试错”的过程，其完全根据环境的“反馈”更新对自己最有利的动作

### 1.6 状态和观测有什么关系？
  - 状态是对环境的完整描述，不会隐藏环境信息。观测是对状态的部分描述，可能会遗漏一些信息。在深度强化学习中，我们几乎总是用同一个实值向量、矩阵或者更高阶的张量来表示状态和观测

### 1.7 根据强化学习智能体的不同，我们可以将其分为哪几类？
  - 基于价值的智能体。显式学习的是价值函数，隐式地学习智能体的策略。因为这个策略是从学到的价值函数里面推算出来的
  - 基于策略的智能体。其直接学习策略，即直接给智能体一个状态，它就会输出对应动作的概率。当然在基于策略的智能体里面并没有去学习智能体的价值函数
  - 另外还有一种智能体，它把以上两者结合。把基于价值和基于策略的智能体结合起来就有了演员-评论员智能体。这一类智能体通过学习策略函数和价值函数以及两者的交互得到更佳的状态

### 1.8 强化学习、监督学习和无监督学习三者有什么区别呢？
  - 首先强化学习和无监督学习是不需要有标签样本的，而监督学习需要许多有标签样本来进行模型的构建训练。其次对于强化学习与无监督学习，无监督学习直接基于给定的数据进行建模，寻找数据或特征中隐藏的结构，一般对应聚类问题；强化学习需要通过延迟奖励学习策略来得到模型与目标的距离，这个距离可以通过奖励函数进行定量判断，这里我们可以将奖励函数视为正确目标的一个稀疏、延迟形式。另外，强化学习处理的多是序列数据，样本之间通常具有强相关性，但其很难像监督学习的样本一样满足独立同分布条件

### 1.9 基于策略迭代和基于价值迭代的强化学习方法有什么区别？
  - 基于策略迭代的强化学习方法，智能体会制定一套动作策略，即确定在给定状态下需要采取何种动作，并根据该策略进行操作。强化学习算法直接对策略进行优化，使得制定的策略能够获得最大的奖励；基于价值迭代的强化学习方法，智能体不需要制定显式的策略，它维护一个价值表格或价值函数，并通过这个价值表格或价值函数来选取价值最大的动作
  - 基于价值迭代的方法只能应用在离散的环境下，例如围棋或某些游戏领域，对于行为集合规模庞大或是动作连续的场景，如机器人控制领域，其很难学习到较好的结果（此时基于策略迭代的方法能够根据设定的策略来选择连续的动作)
  - 基于价值迭代的强化学习算法有 Q-learning、Sarsa 等，基于策略迭代的强化学习算法有策略梯度算法等
  - 此外，演员-评论员算法同时使用策略和价值评估来做出决策。其中，智能体会根据策略做出动作，而价值函数会对做出的动作给出价值，这样可以在原有的策略梯度算法的基础上加速学习过程，从而取得更好的效果

### 1.10 Model-based 和 Model-free、 On Policy 和 Off Policy、Online and Offline、Value-based 和 Policy-based
  - Online和Offline，主要的区别在于智能体训练时是否实时与环境进行交互。Online RL 依赖于实时交互，而 Offline RL 则依赖于预先收集的数据（根据数据稀缺程度选择）
  - On-policy 和Off-policy皆属于Online RL，主要的区别在于是否使用与当前策略相同的数据来进行学习。On-policy 仅使用当前策略产生的数据来更新策略，而 Off-policy 可以使用其他策略生成的数据来学习
  - 在强化学习中，所谓的“模型”一般都指的是环境的模型，即环境的动态模型，通常包含两部分：一是状态转移（state transition）函数，二是奖励（reward）函数

### 1.11 强化学习中所谓的损失函数与深度学习中的损失函数有什么区别呢？
  - 深度学习中的损失函数的目的是使预测值和真实值之间的差距尽可能小，而强化学习中的损失函数的目的是使总奖励的期望尽可能大

# 马尔可夫决策过程完全指南

## 1. 核心概念

### 1.1 基础定义

#### 马尔可夫性质 (Markov Property)
- 系统的未来状态仅取决于当前状态，与过去状态无关
- 数学表达：$P(s_{t+1}|s_t) = P(s_{t+1}|s_1,s_2,...,s_t)$

#### 马尔可夫链 (Markov Chain)
- 具有马尔可夫性质的离散随机过程
- 存在于离散的指数集和状态空间内
- 由状态空间和状态转移矩阵完全定义

#### 状态转移矩阵 (State Transition Matrix)
- 描述状态间转移的条件概率
- 矩阵每行表示从当前状态到所有可能状态的转移概率
- 每行概率和为1

### 1.2 进阶概念

#### 马尔可夫奖励过程 (MRP)
- 马尔可夫链 + 奖励函数
- 组成：$<S,P,R,\gamma>$
  - $S$：状态空间
  - $P$：状态转移概率
  - $R$：奖励函数
  - $\gamma$：折扣因子

#### 马尔可夫决策过程 (MDP)
- MRP + 动作空间
- 组成：$<S,A,P,R,\gamma>$
  - $A$：动作空间
- 比MRP多了决策能力

### 1.3 价值函数

#### 状态价值函数
```math
V_{\pi}(s) = \sum_{a}\pi(a|s)\sum_{s',r}p(s',r|s,a)[r(s,a)+\gamma V_{\pi}(s')]
```

#### 动作价值函数 (Q函数)
```math
Q_{\pi}(s,a)=\sum_{s',r}p(s',r|s,a)r(s',a)+\gamma V_{\pi}(s')
```

## 2. 折扣因子的重要性

### 2.1 为什么需要折扣因子？
1. 避免环状过程中的无穷奖励
2. 体现时间偏好，即更早获得的奖励更有价值
3. 处理不确定性，近期奖励比远期更确定
4. 可作为超参数调节智能体行为

### 2.2 折扣因子的特殊值
- $\gamma = 0$：只关注即时奖励
- $\gamma = 1$：所有时间步的奖励等价
- $0 < \gamma < 1$：折扣未来奖励

## 3. 求解方法

### 3.1 主要方法
1. **动态规划法**
   - 迭代求解贝尔曼方程
   - 收敛条件：状态更新差值小于阈值
   - 适用于模型已知的情况

2. **蒙特卡洛法**
   - 通过采样获得经验
   - 通过多次采样估计期望
   - 不需要完整模型，但需要完整轨迹

3. **时序差分学习**
   - 结合动态规划和蒙特卡洛的优点
   - 可以在线学习
   - 不需要完整模型和完整轨迹

### 3.2 贝尔曼方程
```math
V(s)=R(s)+ \gamma \sum_{s' \in S}p(s'|s)V(s')
```

矩阵形式：
```math
\mathbf{V}=\mathbf{R}+\gamma \mathbf{PV}
```

## 4. 特殊情况处理

### 4.1 非马尔可夫性处理
当数据不满足马尔可夫性质时：
1. 使用循环神经网络(RNN)建模历史信息
2. 应用注意力机制处理历史状态
3. 构建包含历史信息的状态表征

### 4.2 最优性
- 最佳价值函数：$V^*(s)=\max_{\pi} V_{\pi}(s)$
- 最佳策略：$\pi^*(s)=\arg\max_{\pi} V_{\pi}(s)$
- 两者等价但可能存在多个最佳策略

## 5. 总结
马尔可夫决策过程是强化学习的理论基础，通过合适的求解方法（动态规划、蒙特卡洛、时序差分）可以找到最优策略。在实际应用中，需要根据具体问题选择合适的方法，并考虑马尔可夫性质的满足程度。


## 🏆 2. 核心算法  


